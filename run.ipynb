{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment._ENV_move import Board\n",
    "from src.agent._AGENT_move import Piece\n",
    "from src.learn._monte_carlo import Monte_carlo\n",
    "from src.learn._q_learning_move import Q_learning_move\n",
    "from src.learn._td import Temporal_difference\n",
    "from src.learn._td_lambda import Temporal_difference_lambda\n",
    "from src.learn._policy_iteration import Policy_iteration\n",
    "from src.learn._exp_td import Expected_temporal_difference\n",
    "import torch\n",
    "\n",
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move chess\n",
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value function for this policy:\n",
      "tensor([[-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1,  0],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-9, -9, -9, -9, -9, -9, -9, -9],\n",
      "        [-9, -9, -9, -9, -9, -9, -9, -9],\n",
      "        [-9, -9, -9, -9, -9, -8, -8, -8],\n",
      "        [-9, -9, -9, -9, -8, -8, -8, -7],\n",
      "        [-9, -9, -9, -8, -8, -7, -6, -5],\n",
      "        [-9, -9, -9, -8, -7, -6, -1, -1],\n",
      "        [-9, -9, -9, -8, -7, -5, -1,  0],\n",
      "        [-9, -9, -9, -8, -7, -5, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "Optimal policy found in 6 steps of policy evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = Policy_iteration(agent=p,env=env)\n",
    "r.policy_iteration(k=1,gamma=1,synchronous=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function for this policy:\n",
      "tensor([[-5, -5, -4, -4, -4, -3, -3, -3],\n",
      "        [-5, -4, -4, -4, -4, -3, -3, -3],\n",
      "        [-5, -4, -4, -3, -3, -3, -3, -3],\n",
      "        [-4, -4, -4, -3, -3, -2, -2, -2],\n",
      "        [-4, -4, -4, -3, -2, -1, -1, -1],\n",
      "        [-4, -4, -3, -3, -2, -1,  0,  0],\n",
      "        [-4, -3, -3, -3, -2, -1,  0,  0],\n",
      "        [-3, -3, -3, -3, -2, -1,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "r = Temporal_difference(p, env)\n",
    "r.sarsa_td(n_episodes=10000, alpha=0.2, gamma=0.9)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "r.visualize_action_function()\n",
    "# r.TD_zero(epsilon=0.1, alpha=0.05, lamb=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected SARSA temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function for this policy:\n",
      "tensor([[-7, -6, -6, -5, -5, -4, -4, -4],\n",
      "        [-6, -6, -5, -5, -5, -4, -4, -4],\n",
      "        [-6, -6, -5, -5, -4, -4, -4, -4],\n",
      "        [-5, -5, -5, -5, -4, -3, -3, -3],\n",
      "        [-5, -5, -5, -4, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -4, -3, -2,  0,  0],\n",
      "        [-4, -4, -4, -4, -3, -2,  0,  0],\n",
      "        [-4, -4, -4, -4, -3, -2,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = Expected_temporal_difference(agent=p, env=env)\n",
    "r.expected_sarsa(n_episodes=10000, alpha=0.2, gamma=0.9)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "r.visualize_action_function()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference lambda (da lasciare?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6, -5, -5, -5, -5, -5, -5, -4],\n",
      "        [-6, -5, -5, -4, -4, -4, -4, -5],\n",
      "        [-6, -5, -4, -4, -3, -3, -3, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -2, -3],\n",
      "        [-5, -5, -4, -4, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2,  0,  0],\n",
      "        [-5, -5, -4, -3, -3, -2,  0,  0],\n",
      "        [-5, -5, -4, -3, -3, -2,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = Temporal_difference_lambda(agent=p, env=env)\n",
    "r.sarsa_lambda(n_episodes=10000,alpha=0.2,gamma=0.9)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "r.visualize_action_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5, -5, -4, -4, -4, -5, -4, -4],\n",
      "        [-5, -4, -4, -4, -4, -4, -4, -5],\n",
      "        [-5, -4, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -4, -4, -3, -2, -2, -2, -2],\n",
      "        [-5, -4, -4, -3, -2, -1, -1, -1],\n",
      "        [-5, -5, -4, -3, -2, -1,  0,  0],\n",
      "        [-5, -5, -4, -3, -2, -1,  0,  0],\n",
      "        [-5, -5, -4, -3, -2, -1,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = Q_learning_move(agent=p, env=env)\n",
    "r.q_learning(n_episodes=10000, alpha=0.2, gamma=0.9)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "r.visualize_action_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-15, -23, -24, -15, -16, -22, -18, -17],\n",
      "        [-19, -21, -19, -11, -19, -17, -18, -17],\n",
      "        [-26, -25, -18, -20,  -8, -11, -13, -12],\n",
      "        [-24, -22, -16, -10,  -9,  -8,  -6,  -8],\n",
      "        [-21, -13, -11,  -8,  -7,  -4,  -3,  -3],\n",
      "        [-15, -17, -10,  -6,  -3,  -4,  -1,  -1],\n",
      "        [-26, -23, -16, -11,  -6,  -3,  -1,   0],\n",
      "        [-19, -10, -17, -11,  -4,  -2,  -1,  -1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = Monte_carlo(agent=p, env=env)\n",
    "for k in range(10000):\n",
    "    eps = 0.5\n",
    "    r.monte_carlo_learning(epsilon=eps)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "\n",
    "print(torch.max(r.agent.action_function, dim=2)[0].to(torch.int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
