{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment._ENV_move import Board\n",
    "from src.agent._AGENT_move import Piece\n",
    "from src.learn._monte_carlo import Monte_carlo\n",
    "from src.learn._q_learning_move import Q_learning_move\n",
    "from src.learn._td import Temporal_difference\n",
    "from src.learn._td_lambda import Temporal_difference_lambda\n",
    "from src.learn._policy_iteration import Policy_iteration\n",
    "from src.learn._exp_td import Expected_temporal_difference\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move chess\n",
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value function for this policy:\n",
      "tensor([[-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [-1, -1, -1, -1, -1, -1, -1,  0],\n",
      "        [-1, -1, -1, -1, -1, -1, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-9, -9, -9, -9, -9, -9, -9, -9],\n",
      "        [-9, -9, -9, -9, -9, -9, -9, -9],\n",
      "        [-9, -9, -9, -9, -9, -8, -8, -8],\n",
      "        [-9, -9, -9, -9, -8, -8, -8, -7],\n",
      "        [-9, -9, -9, -8, -8, -7, -6, -5],\n",
      "        [-9, -9, -9, -8, -7, -6, -1, -1],\n",
      "        [-9, -9, -9, -8, -7, -5, -1,  0],\n",
      "        [-9, -9, -9, -8, -7, -5, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "\n",
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -5, -5, -5, -5, -5],\n",
      "        [-5, -5, -4, -4, -4, -4, -4, -4],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1],\n",
      "        [-5, -5, -4, -3, -3, -2, -1,  0],\n",
      "        [-5, -5, -4, -3, -3, -2, -1, -1]], dtype=torch.int32)\n",
      "Optimal policy found in 6 steps of policy evaluation\n"
     ]
    }
   ],
   "source": [
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "r = Policy_iteration(agent=p,env=env)\n",
    "r.policy_iteration(k=1,gamma=1,synchronous=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at episode: 372\n",
      "[['↘', '↘', '↑', '←', '←', '↖', '←', '↙'],\n",
      " ['↓', '↘', '↘', '↘', '↑', '↘', '↑', '↙'],\n",
      " ['↙', '↘', '↘', '↘', '↙', '↓', '↙', '↗'],\n",
      " ['→', '↘', '↓', '↘', '↘', '↘', '↓', '↙'],\n",
      " ['↖', '←', '↖', '↘', '→', '↘', '↘', '↑'],\n",
      " ['↙', '↘', '→', '↖', '→', '↘', '↘', '↓'],\n",
      " ['↙', '↙', '↑', '↗', '→', '←', '→', 'F'],\n",
      " ['↓', '↑', '→', '↙', '↗', '↗', '↗', '↑']]\n",
      "Value function for this policy:\n",
      "tensor([[-5, -4, -4, -3, -3, -3, -3, -3],\n",
      "        [-4, -4, -4, -3, -3, -3, -3, -2],\n",
      "        [-4, -4, -4, -3, -3, -3, -2, -2],\n",
      "        [-4, -3, -3, -3, -2, -2, -2, -2],\n",
      "        [-3, -3, -3, -3, -2, -1, -1, -1],\n",
      "        [-3, -3, -3, -3, -2, -1,  0,  0],\n",
      "        [-3, -3, -3, -2, -2, -1,  0,  0],\n",
      "        [-3, -3, -3, -2, -2, -1,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "td = Temporal_difference(p, env)\n",
    "td.sarsa_td(n_episodes=1000, alpha=0.2, gamma=0.9)\n",
    "td.visualize_policy() # controllare il risultato\n",
    "td.visualize_action_function()\n",
    "# r.TD_zero(epsilon=0.1, alpha=0.05, lamb=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected SARSA temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function for this policy:\n",
      "tensor([[-5, -5, -5, -4, -4, -3, -3, -3],\n",
      "        [-5, -5, -4, -4, -4, -3, -3, -3],\n",
      "        [-5, -4, -4, -4, -3, -3, -3, -3],\n",
      "        [-4, -4, -4, -4, -3, -3, -2, -2],\n",
      "        [-4, -4, -4, -3, -3, -2, -2, -2],\n",
      "        [-4, -4, -3, -3, -2, -2,  0,  0],\n",
      "        [-4, -3, -3, -3, -2, -2,  0,  0],\n",
      "        [-3, -3, -3, -3, -2, -2,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "exp_tp = Expected_temporal_difference(agent=p, env=env)\n",
    "exp_tp.expected_sarsa(n_episodes=1000, alpha=0.2, gamma=0.9)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "exp_tp.visualize_action_function()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of Policy Difference: 12.506937980651855\n"
     ]
    }
   ],
   "source": [
    "policy_diff = torch.norm(td.agent.action_function - exp_tp.agent.action_function)\n",
    "print(\"Norm of Policy Difference:\", policy_diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference lambda (da lasciare?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6, -5, -5, -5, -5, -4, -4, -3],\n",
      "        [-6, -5, -5, -5, -5, -4, -4, -3],\n",
      "        [-6, -5, -5, -4, -3, -3, -3, -3],\n",
      "        [-5, -5, -4, -4, -3, -2, -3, -2],\n",
      "        [-5, -5, -4, -3, -3, -2, -2, -2],\n",
      "        [-4, -4, -4, -3, -2, -2,  0,  0],\n",
      "        [-4, -4, -4, -3, -2, -2,  0,  0],\n",
      "        [-4, -4, -4, -3, -2, -2,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "td_lambda = Temporal_difference_lambda(agent=p, env=env)\n",
    "td_lambda.sarsa_lambda(n_episodes=1000,alpha=0.2,gamma=0.9)\n",
    "# td_lambda.visualize_policy() # controllare il risultato\n",
    "td_lambda.visualize_action_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of Policy Difference: 30.821823120117188\n"
     ]
    }
   ],
   "source": [
    "policy_diff = torch.norm(td.agent.action_function - td_lambda.agent.action_function)\n",
    "print(\"Norm of Policy Difference:\", policy_diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5, -4, -4, -4, -3, -3, -3, -3],\n",
      "        [-4, -4, -4, -4, -3, -3, -3, -3],\n",
      "        [-4, -4, -4, -3, -3, -3, -3, -2],\n",
      "        [-4, -4, -3, -3, -2, -2, -2, -2],\n",
      "        [-3, -3, -3, -3, -2, -1, -1, -1],\n",
      "        [-3, -3, -3, -3, -2, -1,  0,  0],\n",
      "        [-3, -3, -3, -3, -2, -1,  0,  0],\n",
      "        [-3, -3, -3, -2, -2, -1,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "q = Q_learning_move(agent=p, env=env)\n",
    "q.q_learning(n_episodes=1000, alpha=0.2, gamma=0.9)\n",
    "# q.visualize_policy() # controllare il risultato\n",
    "q.visualize_action_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of Policy Difference: 5.995652198791504\n"
     ]
    }
   ],
   "source": [
    "policy_diff = torch.norm(td.agent.action_function - q.agent.action_function)\n",
    "print(\"Norm of Policy Difference:\", policy_diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-20, -53, -36, -15, -35, -15, -24, -14],\n",
      "        [-21, -40, -19,  -9, -13, -26, -26, -15],\n",
      "        [-30, -19, -28, -13,  -8, -20, -12, -14],\n",
      "        [-26, -17, -26,  -7,  -9,  -8,  -6,  -4],\n",
      "        [-19, -29,  -7, -18,  -3,  -8,  -4,  -2],\n",
      "        [-26, -23, -24,  -6,  -9,  -3,  -1,  -1],\n",
      "        [-28, -12, -13, -18,  -8,  -7,  -1,   0],\n",
      "        [-35, -29,  -5, -24,  -3,  -3,  -1,  -1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "env = Board()\n",
    "p = Piece(piece='king')  # king\", \"rook\", \"bishop\" or \"knight\"\n",
    "mc = Monte_carlo(agent=p, env=env)\n",
    "for k in range(1000):\n",
    "    eps = 0.5\n",
    "    mc.monte_carlo_learning(epsilon=eps)\n",
    "# r.visualize_policy() # controllare il risultato\n",
    "\n",
    "print(torch.max(mc.agent.action_function, dim=2)[0].to(torch.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of Policy Difference: 988.563232421875\n"
     ]
    }
   ],
   "source": [
    "policy_diff = torch.norm(td.agent.action_function - mc.agent.action_function)\n",
    "print(\"Norm of Policy Difference:\", policy_diff.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
